#!/usr/bin/env python3
"""
Integration Tests for Borg.tools Scanner v2.0

Tests the complete scan pipeline from start to finish including:
- Full scan pipeline
- Deep scan mode
- LLM integration (dry run with mocks)
- Agent Zero integration (optional)
- Output validation

Run: pytest tests/integration_test.py -v
"""

import json
import os
import shutil
import tempfile
from pathlib import Path
from typing import Dict, Any
from unittest.mock import patch, MagicMock

import pytest


# Test fixtures directory
FIXTURES_DIR = Path(__file__).parent / "fixtures"


@pytest.fixture
def temp_project():
    """Create a temporary test project"""
    with tempfile.TemporaryDirectory() as tmpdir:
        project_path = Path(tmpdir) / "test_project"
        project_path.mkdir()

        # Create Python files
        (project_path / "main.py").write_text("""
def hello_world():
    '''Say hello'''
    print("Hello, World!")

if __name__ == "__main__":
    hello_world()
""")

        (project_path / "utils.py").write_text("""
def add(a, b):
    return a + b

def multiply(a, b):
    return a * b
""")

        # Create README
        (project_path / "README.md").write_text("""
# Test Project

A simple test project for integration testing.

## Installation

```bash
pip install -r requirements.txt
```

## Usage

```bash
python main.py
```
""")

        # Create requirements.txt
        (project_path / "requirements.txt").write_text("""
pytest==7.4.0
requests==2.31.0
""")

        # Create tests directory
        (project_path / "tests").mkdir()
        (project_path / "tests" / "test_main.py").write_text("""
import pytest
from main import hello_world

def test_hello_world():
    hello_world()
""")

        # Create .github/workflows for CI
        (project_path / ".github").mkdir()
        (project_path / ".github" / "workflows").mkdir()
        (project_path / ".github" / "workflows" / "ci.yml").write_text("""
name: CI
on: [push]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - run: pytest
""")

        yield project_path


@pytest.fixture
def scanner_root():
    """Get scanner root directory"""
    return Path(__file__).parent.parent


def test_full_scan_pipeline(temp_project, scanner_root):
    """Test complete scan from start to finish"""
    import sys
    sys.path.insert(0, str(scanner_root))

    from borg_tools_scan import scan_project, render_report_md

    # Run scanner on fixture project
    summary = scan_project(temp_project)

    # Verify Facts collected
    assert summary.facts.name == "test_project"
    assert "python" in summary.facts.languages
    assert summary.facts.has_readme is True
    assert summary.facts.has_tests is True
    assert summary.facts.has_ci is True

    # Verify Scores calculated
    assert summary.scores.stage in ["idea", "prototype", "mvp", "beta", "prod"]
    assert 0 <= summary.scores.value_score <= 10
    assert 0 <= summary.scores.risk_score <= 10
    assert 0 <= summary.scores.priority <= 20

    # Verify Suggestions generated
    assert isinstance(summary.suggestions.todo_now, list)
    assert isinstance(summary.suggestions.todo_next, list)
    assert summary.suggestions.rationale != ""

    # Verify report generation
    report = render_report_md(summary)
    assert "# test_project" in report
    assert "Etap i ocena" in report
    assert "TODO" in report


def test_deep_scan_mode(temp_project, scanner_root):
    """Test deep scan with all analyzers"""
    import sys
    sys.path.insert(0, str(scanner_root))
    sys.path.insert(0, str(scanner_root / "modules"))

    from modules.code_analyzer import analyze_code
    from modules.deployment_detector import detect_deployment
    from modules.doc_analyzer import analyze_documentation

    # Test code analyzer
    code_result = analyze_code(
        project_path=str(temp_project),
        languages=["python"]
    )

    # Code analyzer returns nested structure with code_quality key
    assert "code_quality" in code_result
    assert "overall_score" in code_result["code_quality"]
    assert "complexity_metrics" in code_result["code_quality"]
    assert 0 <= code_result["code_quality"]["overall_score"] <= 10

    # Test deployment detector
    deploy_result = detect_deployment(
        project_path=str(temp_project),
        languages=["python"],
        facts={"languages": ["python"]}
    )

    # Deployment detector returns nested structure with deployment key
    assert "deployment" in deploy_result
    assert "deployment_type" in deploy_result["deployment"]
    assert "is_deployable" in deploy_result["deployment"]
    assert "readiness_score" in deploy_result["deployment"]

    # Test doc analyzer
    doc_result = analyze_documentation(
        project_path=str(temp_project),
        languages=["python"],
        facts={}
    )

    # Doc analyzer returns nested structure with documentation key
    assert "documentation" in doc_result
    assert "overall_score" in doc_result["documentation"]
    assert "found_docs" in doc_result["documentation"]
    assert doc_result["documentation"]["found_docs"]["readme"]["exists"] is True


def test_output_validation(temp_project, scanner_root):
    """Validate all output files and JSON schemas"""
    import sys
    sys.path.insert(0, str(scanner_root))

    from borg_tools_scan import scan_project
    import dataclasses

    summary = scan_project(temp_project)

    # Test JSON serialization
    summary_dict = {
        "facts": dataclasses.asdict(summary.facts),
        "scores": dataclasses.asdict(summary.scores),
        "suggestions": dataclasses.asdict(summary.suggestions)
    }

    # Verify can serialize to JSON
    json_str = json.dumps(summary_dict, ensure_ascii=False)
    assert json_str is not None

    # Parse back
    parsed = json.loads(json_str)
    assert "facts" in parsed
    assert "scores" in parsed
    assert "suggestions" in parsed

    # Validate facts schema
    assert "name" in parsed["facts"]
    assert "languages" in parsed["facts"]
    assert isinstance(parsed["facts"]["languages"], list)

    # Validate scores schema
    assert "stage" in parsed["scores"]
    assert "value_score" in parsed["scores"]
    assert "risk_score" in parsed["scores"]
    assert "priority" in parsed["scores"]

    # Validate suggestions schema
    assert "todo_now" in parsed["suggestions"]
    assert "todo_next" in parsed["suggestions"]
    assert isinstance(parsed["suggestions"]["todo_now"], list)


def test_vibesummary_generation(temp_project, scanner_root):
    """Test VibeSummary.md generation"""
    import sys
    sys.path.insert(0, str(scanner_root))
    sys.path.insert(0, str(scanner_root / "modules"))

    from modules.vibesummary_generator import VibeSummaryGenerator
    from modules.code_analyzer import analyze_code
    from modules.deployment_detector import detect_deployment
    from modules.doc_analyzer import analyze_documentation

    # Collect all analysis data
    code_analysis = analyze_code(str(temp_project), ["python"])
    deployment_analysis = detect_deployment(str(temp_project), ["python"], {"languages": ["python"]})
    doc_analysis = analyze_documentation(str(temp_project), ["python"], {})

    # Mock LLM results
    llm_results = {
        "architect": {
            "architecture_assessment": "Simple modular structure",
            "design_patterns": ["Functional"],
            "scalability_notes": "Good for small projects",
            "technical_debt_priority": "low"
        },
        "business": {
            "problem_solved": "Example project",
            "target_audience": "Developers",
            "market_viability": 5,
            "monetization_strategy": "N/A",
            "portfolio_suitable": False,
            "portfolio_pitch": ""
        },
        "deployment": {
            "deployment_strategy": "docker",
            "infrastructure_recommendations": "Use Docker for deployment",
            "mvp_roadmap": ["Add Dockerfile", "Setup CI/CD"]
        },
        "aggregator": {
            "overall_assessment": "Good starting point",
            "vibecodibility_score": 6,
            "top_priorities": ["Add Dockerfile", "Improve tests"],
            "borg_tools_fit": 5
        }
    }

    # Generate VibeSummary
    generator = VibeSummaryGenerator()

    # Prepare project summary dict
    project_summary = {
        "project_path": str(temp_project),
        "project_name": "test_project",
        "languages": ["python"],
        "code_analysis": code_analysis,
        "deployment": deployment_analysis,
        "documentation": doc_analysis,
        "llm_results": llm_results
    }

    # Generate to temp file
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
        output_path = Path(f.name)

    success = generator.generate_vibesummary(project_summary, output_path)
    assert success is True

    # Read generated file
    vibesummary = output_path.read_text()

    # Clean up
    output_path.unlink()

    # Verify VibeSummary structure
    assert "# VibeSummary: test_project" in vibesummary
    assert "## Project Essence" in vibesummary
    assert "## Vibecodibility Scores" in vibesummary
    assert "## Architecture & Design" in vibesummary
    assert "## Deployment Status" in vibesummary
    assert "## Documentation Quality" in vibesummary
    assert "## Actionable Next Steps" in vibesummary


@patch("urllib.request.urlopen")
def test_llm_pipeline_dry_run(mock_urlopen, temp_project, scanner_root):
    """Test LLM integration with mocked responses"""
    import sys
    sys.path.insert(0, str(scanner_root))
    sys.path.insert(0, str(scanner_root / "modules"))

    from modules.llm_orchestrator import ModelPipeline

    # Mock LLM response
    mock_response = MagicMock()
    mock_response.read.return_value = json.dumps({
        "choices": [{
            "message": {
                "content": json.dumps({
                    "architecture_assessment": "Test assessment",
                    "design_patterns": ["MVC"],
                    "scalability_notes": "Scalable",
                    "technical_debt_priority": "low"
                })
            }
        }]
    }).encode()
    mock_response.__enter__.return_value = mock_response
    mock_urlopen.return_value = mock_response

    # Create pipeline - check if it needs different args
    try:
        # Try creating pipeline without api_key
        pipeline = ModelPipeline()

        # Test project data
        project_data = {
            "name": "test_project",
            "path": str(temp_project),
            "languages": ["python"],
            "code_analysis": {"overall_score": 7.5},
            "deployment": {"deployment_type": "unknown"},
            "documentation": {"overall_score": 6.0}
        }

        # Run analysis (will use mocked response)
        results = pipeline.run_parallel_analysis(project_data)

        # Verify results structure
        assert isinstance(results, dict)
        # Results might have architect, business, deployment, aggregator keys
    except Exception as e:
        # LLM call might fail in test environment - that's OK
        pytest.skip(f"LLM call failed (expected in test): {e}")


@patch("urllib.request.urlopen")
def test_agent_zero_integration(mock_urlopen, temp_project):
    """Test Agent Zero integration (if available)"""
    # Check if BORG_TOOLS_API_KEY is set
    if not os.getenv("BORG_TOOLS_API_KEY"):
        pytest.skip("BORG_TOOLS_API_KEY not set - skipping Agent Zero test")

    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent))
    sys.path.insert(0, str(Path(__file__).parent.parent / "modules"))

    from modules.agent_zero_bridge import AgentZeroBridge

    # Mock successful audit response
    mock_response = MagicMock()
    mock_response.read.return_value = json.dumps({
        "audit_id": "test-audit-123",
        "status": "completed",
        "workflows": [
            {
                "type": "code_review",
                "status": "success",
                "bonus_points": 0.5
            }
        ],
        "total_bonus": 0.5
    }).encode()
    mock_response.getcode.return_value = 200
    mock_response.__enter__.return_value = mock_response
    mock_urlopen.return_value = mock_response

    # Initialize bridge
    bridge = AgentZeroBridge(api_key="test-key")

    # Submit test audit (will use mocked response)
    try:
        result = bridge.submit_audit(
            project_path=str(temp_project),
            workflows=["code_review"],
            context={"language": "python"}
        )

        # Verify bonus scoring
        assert "total_bonus" in result
        assert isinstance(result["total_bonus"], (int, float))
    except Exception as e:
        # Agent Zero might not be reachable - that's OK
        pytest.skip(f"Agent Zero not reachable (expected): {e}")


def test_cache_system(temp_project, scanner_root):
    """Test cache manager functionality"""
    import sys
    sys.path.insert(0, str(scanner_root))
    sys.path.insert(0, str(scanner_root / "modules"))

    from modules.cache_manager import CacheManager

    # Create cache manager with in-memory DB
    cache = CacheManager(db_path=":memory:")

    # Test cache write using set_cache method
    cache.set_cache(
        project_path=str(temp_project),
        model_name="test_model",
        response={"result": "test"}
    )

    # Test cache read using get_cached method
    cached = cache.get_cached(str(temp_project), "test_model")
    assert cached is not None
    assert cached["result"] == "test"

    # Test cache stats
    stats = cache.get_stats()
    assert isinstance(stats, dict)

    # Modify file to test invalidation
    (temp_project / "main.py").write_text("# Modified")

    # Cache might be invalidated depending on implementation
    # Just verify it doesn't crash
    cached_after = cache.get_cached(str(temp_project), "test_model")


def test_progress_reporter(scanner_root):
    """Test progress reporter UI"""
    import sys
    sys.path.insert(0, str(scanner_root))
    sys.path.insert(0, str(scanner_root / "modules"))

    try:
        from modules.progress_reporter import ProgressReporter

        # Create reporter
        reporter = ProgressReporter()

        # Test basic operations
        reporter.start_project("test_project", 1, 5)
        reporter.log_step("ðŸ”", "Scanning...", "cyan")
        reporter.show_progress_bar(50, 100, "Files")
        reporter.complete_project({
            "stage": "mvp",
            "value_score": 7.5,
            "risk_score": 3.2,
            "priority": 14
        })

        # Should not crash
        assert True

    except ImportError:
        pytest.skip("rich library not available - skipping UI test")


def test_error_handling(temp_project, scanner_root):
    """Test error handling and graceful degradation"""
    import sys
    sys.path.insert(0, str(scanner_root))

    from borg_tools_scan import scan_project

    # Create project with problematic files
    (temp_project / "bad.py").write_text("import invalid syntax here")

    # Scanner should still complete without crashing
    try:
        summary = scan_project(temp_project)
        assert summary is not None
    except Exception as e:
        pytest.fail(f"Scanner crashed on bad input: {e}")


def test_multi_language_support(scanner_root):
    """Test support for multiple languages"""
    import sys
    sys.path.insert(0, str(scanner_root))

    from borg_tools_scan import detect_languages

    with tempfile.TemporaryDirectory() as tmpdir:
        project = Path(tmpdir)

        # Create multi-language project
        (project / "app.py").write_text("print('Python')")
        (project / "script.js").write_text("console.log('JavaScript')")
        (project / "package.json").write_text('{"name": "test"}')
        (project / "main.go").write_text("package main")
        (project / "go.mod").write_text("module test")

        languages = detect_languages(project)

        assert "python" in languages
        # NodeJS detection requires package.json to exist
        assert "nodejs" in languages or "javascript-typescript" in languages or len(languages) >= 2
        assert "go" in languages


def test_scoring_consistency(temp_project, scanner_root):
    """Test that scoring is deterministic"""
    import sys
    sys.path.insert(0, str(scanner_root))

    from borg_tools_scan import scan_project

    # Scan twice
    summary1 = scan_project(temp_project)
    summary2 = scan_project(temp_project)

    # Scores should be identical
    assert summary1.scores.value_score == summary2.scores.value_score
    assert summary1.scores.risk_score == summary2.scores.risk_score
    assert summary1.scores.priority == summary2.scores.priority
    assert summary1.scores.stage == summary2.scores.stage


if __name__ == "__main__":
    # Run tests with pytest
    pytest.main([__file__, "-v", "--tb=short"])
